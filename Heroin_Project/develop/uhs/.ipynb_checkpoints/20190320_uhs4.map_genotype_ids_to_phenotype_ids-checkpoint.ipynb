{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map genotype ids to phenotype ids\n",
    "The individual IDs are made up of different portions (e.g. AS00-00347_8002022294_HHG10078_12_H06). To map these genotype IDs to the phenotype IDs is not straight forward. In particular, some of the IDs we might be able to map to the phenotype file by the cell_line portion (e.g. HHG10078) while other times we might have to try to map to the phenotype file using the serum ID (e.g. AS00-00347). The follow script does this searching for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of HA subjects with HIV\n",
    "We need to know what the proportion of HA subjects have HIV when we consider the subjects classified as HA during the STRUCTURE analysis when the standad 25% threshold was used and also when the 8% threshold was used.\n",
    "\n",
    "We will need a list of those subjects. We created these two lists and copied them to our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut -f2 ha_filtered.0.25 | xargs -I{} grep {} genotype.to.phenotype.map2 > genotype.to.phenotype.ha.0.25 &\n",
    "cut -f2 ha_filtered.0.08 | xargs -I{} grep {} genotype.to.phenotype.map2 > genotype.to.phenotype.ha.0.08 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'serum' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b2d115b6870d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0moutF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mmap_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m#    def ha_filter(ha_ids, map_file):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b2d115b6870d>\u001b[0m in \u001b[0;36mmap_fun\u001b[1;34m(gen, phen, match_var)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0masF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpF\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mphead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mserum_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"serum\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mcell_line_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cell_line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mgwas_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gwasserum\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'serum' is not in list"
     ]
    }
   ],
   "source": [
    "### python ###\n",
    "import itertools, os\n",
    "\n",
    "#os.chdir(\"/Users/jmarks/OneDrive - Research Triangle Institute/Projects/heroin/ngc/uhs4/phenotype\")\n",
    "#print(os.getcwd())\n",
    "\n",
    "base_dir = \"/Users/jmarks/OneDrive - Research Triangle Institute/Projects/heroin/ngc/uhs4/phenotype\"\n",
    "date = \"20190320\"\n",
    "ha_percent = \"8\"\n",
    "match_list = [\"viralload_cperml.y\", \"viralload_cperml.x\", \"hiv_status\", \"gwashiv\", \"hivstat\", \"hiv\"]\n",
    "for match_var in match_list:\n",
    "    \n",
    "    gen = \"{}/processing/master.genotype.ids.n3469\".format(base_dir)\n",
    "    phen = \"{}/unprocessed/hiv_all_merged_with_uhs_all_phenotype_data_08282017.csv\".format(base_dir)\n",
    "    out_file = \"{}/processing/{}.genotype.to.phenotype.ancestry.{}.ha_{}percent.map\".format(base_dir,  date, match_var, ha_percent)\n",
    "    ha_ids = \"ha.ids.{}\".format(ha_percent)\n",
    "\n",
    "    def glance(d):\n",
    "        return dict(itertools.islice(d.items(), 3))\n",
    "\n",
    "    def map_fun(gen, phen, match_var):\n",
    "        with open(gen) as asF, open(phen) as pF:\n",
    "            phead = pF.readline().split(\",\")\n",
    "            serum_index = phead.index(\"serum\")\n",
    "            cell_line_index = phead.index(\"cell_line\")\n",
    "            gwas_index = phead.index(\"gwasserum\")\n",
    "            ancestry_index = phead.index(\"ancestry_selfreport\")\n",
    "            hiv_index = phead.index(match_var)\n",
    "\n",
    "            cell_dic = {}\n",
    "            serum_dic = {}\n",
    "            gwas_dic = {}\n",
    "            line = pF.readline()\n",
    "            while line:\n",
    "                sl = line.split(\",\")\n",
    "                cell_dic[sl[cell_line_index]] = (phead[cell_line_index], sl[ancestry_index], sl[hiv_index])\n",
    "                serum_dic[sl[serum_index]] = (phead[serum_index], sl[ancestry_index], sl[hiv_index])\n",
    "                gwas_dic[sl[gwas_index]] = (phead[gwas_index], sl[ancestry_index], sl[hiv_index])\n",
    "                line = pF.readline()\n",
    "            print(glance(cell_dic))\n",
    "            print(glance(gwas_dic))\n",
    "        #\n",
    "            keep_list = []\n",
    "            sline = asF.readline()\n",
    "            while sline:\n",
    "                spl = sline.split()\n",
    "                if spl[1] in cell_dic:\n",
    "                    tmptup = (spl[2], cell_dic[spl[1]])\n",
    "                    keep_list.append(tmptup)\n",
    "                elif spl[0] in serum_dic:\n",
    "                    tmptup = (spl[2], serum_dic[spl[0]])\n",
    "                    keep_list.append(tmptup)\n",
    "                elif spl[0] in gwas_dic:\n",
    "                    tmptup = (spl[2], gwas_dic[spl[0]])\n",
    "                    keep_list.append(tmptup)\n",
    "                else:\n",
    "                    print(spl[2])\n",
    "                sline = asF.readline()\n",
    "\n",
    "\n",
    "        print(len(keep_list))\n",
    "        mytup = keep_list[1]\n",
    "        mytup = (mytup[0],) + mytup[1]\n",
    "\n",
    "        mapped_ids = [(x[0],) + x[1] for x in keep_list]\n",
    "        print(mapped_ids[:5])\n",
    "\n",
    "\n",
    "        out_head = \"{}\\t{}\\t{}\\t{}\".format(\"genotype_id\", \"phenotype_column\", \"ancestry_selfreport\", match_var)\n",
    "        with open(out_file, 'w') as outF:\n",
    "            outF.write(out_head + \"\\n\")\n",
    "            for x in mapped_ids:\n",
    "                line = \"\\t\".join(str(i) for i in x)\n",
    "                outF.write(line + \"\\n\")\n",
    "            print(\"done\")\n",
    "    map_fun(gen, phen, match_var)\n",
    "    \n",
    "#    def ha_filter(ha_ids, map_file):\n",
    "#        out_file2 = \"{}.ha_only\".format(map_file)\n",
    "#        with open(ha_ids) as inF, open(map_file) as mF, open(out_file2, \"w\") as outF:\n",
    "#            head = mF.readline()\n",
    "#            outF.write(head)\n",
    "#            data_dic = {}\n",
    "#            line = mF.readline()\n",
    "#            while line:\n",
    "#                sl = line.split()\n",
    "#                data_dic[sl[0]] = line\n",
    "#                line = mF.readline()\n",
    "#\n",
    "#            line = inF.readline()\n",
    "#            while line:\n",
    "#                sl = line.strip()\n",
    "#                outF.write(data_dic[sl])\n",
    "#                line = inF.readline()\n",
    "#\n",
    "#    ha_filter(ha_ids, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jmarks/OneDrive - Research Triangle Institute/Projects/heroin/ngc/uhs4/phenotype/ha_data/20190313.genotype.to.phenotype.ancestry.viralload_cperml.x.ha_8percent.map'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ha_filter(ha_ids, map_file):\n",
    "    out_file2 = \"{}.ha_only\".format(map_file)\n",
    "    with open(ha_ids) as inF, open(map_file) as mF, open(out_file2, \"w\") as outF:\n",
    "        head = mF.readline()\n",
    "        outF.write(head)\n",
    "        data_dic = {}\n",
    "        line = mF.readline()\n",
    "        while line:\n",
    "            sl = line.split()\n",
    "            data_dic[sl[0]] = line\n",
    "            line = mF.readline()\n",
    "\n",
    "        line = inF.readline()\n",
    "        while line:\n",
    "            sl = line.strip()\n",
    "            outF.write(data_dic[sl])\n",
    "            line = inF.readline()\n",
    "        \n",
    "ha_filter(ha_ids, out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable summary\n",
    "### 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of HA classified subjects being HIV cases using 25% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.hiv.ha_25percent.map.ha_only| ww\n",
    "\"\"\"23\"\"\"\n",
    "\n",
    "# number of HA classified subjects being HIV cases using 25% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.hivstat.ha_25percent.map.ha_only| ww\n",
    "\"\"\"23\"\"\"\n",
    "\n",
    "# number of HA classified subjects being HIV cases using 25% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.hiv_status.ha_25percent.map.ha_only| ww\n",
    "\"\"\"23\"\"\"\n",
    "\n",
    "# number of HA classified subjects being HIV cases using 25% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.gwashiv.ha_25percent.map.ha_only| ww\n",
    "\"\"\"23\"\"\"\n",
    "\n",
    "# number of HA classified subjects with viral load using 25% threshold for the ancestry cutoff\n",
    "awk '$4!=-9' 20190313.genotype.to.phenotype.ancestry.viralload_cperml.y.ha_25percent.map.ha_only| ww\n",
    "\"\"\"23\"\"\"\n",
    "\n",
    "# number of HA classified subjects with viral load using 25% threshold for the ancestry cutoff\n",
    "awk '$4!~\"NA\"' 20190313.genotype.to.phenotype.ancestry.viralload_cperml.x.ha_25percent.map.ha_only | ww\n",
    "\"\"\"23\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of HA classified subjects being HIV cases using 8% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.hiv.ha_8percent.map.ha_only| ww\n",
    "#\"\"\"71\"\"\"\n",
    "\n",
    "# number of HA classified subjects being HIV cases using 8% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.hivstat.ha_8percent.map.ha_only| ww\n",
    "#\"\"\"71\"\"\"\n",
    "\n",
    "# number of HA classified subjects being HIV cases using 8% threshold for the ancestry cutoff\n",
    "awk '$4==1' 20190313.genotype.to.phenotype.ancestry.hiv_status.ha_8percent.map.ha_only| ww\n",
    "#\"\"\"71\"\"\"\n",
    "\n",
    "# number of HA classified subjects being HIV cases using 8% threshold for the ancestry cutoff\n",
    "awk '$4~1' 20190313.genotype.to.phenotype.ancestry.gwashiv.ha_8percent.map.ha_only| ww\n",
    "#\"\"\"0\"\"\"\n",
    "\n",
    "# number of HA classified subjects with viral load using 8% threshold for the ancestry cutoff\n",
    "awk '$4!=-9' 20190313.genotype.to.phenotype.ancestry.viralload_cperml.y.ha_8percent.map.ha_only| ww\n",
    "#\"\"\"66\"\"\"\n",
    "\n",
    "# number of HA classified subjects with viral load using 8% threshold for the ancestry cutoff\n",
    "awk '$4!~\"NA\"' 20190313.genotype.to.phenotype.ancestry.viralload_cperml.x.ha_8percent.map.ha_only | ww\n",
    "#\"\"\"70\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3469\n",
      "[('\"AS00-00347_8002022294_HHG10078_12_H06\"', '\"cell_line\"', '\"3\"', '\"-9\"', '\"49\"', '\"1\"'), ('\"AS00-00351_8002220319_HHG6146_36_C02\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"35\"', '\"2\"'), ('\"AS00-00437_8002220343_HHG6150_36_D02\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"41\"', '\"1\"'), ('\"AS00-00458_8002694957_HHG0612_1_D01\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"50\"', '\"1\"'), ('\"AS00-00459_8002220355_HHG6152_36_E02\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"52\"', '\"2\"')]\n",
      "done\n",
      "\"viralload_log10.y\"\n"
     ]
    }
   ],
   "source": [
    "### python ###\n",
    "import itertools, os\n",
    "\"\"\"\n",
    "This function will parse the master phenotype file and map the genotype IDs to the\n",
    "corresponding phenotype IDs. This was developed because there is no straight forward \n",
    "way to map the gentype IDs to the phenotype file. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "base_dir = \"/Users/jmarks/OneDrive - Research Triangle Institute/Projects/heroin/ngc/uhs4/phenotype\"\n",
    "date = \"20190321\"\n",
    "ha_percent = \"8\"\n",
    "match_var = '\"viralload_log10.y\"' # phenotype variable of interest in the master phenotype file\n",
    "#match_list = [\"viralload_cperml.y\", \"viralload_cperml.x\", \"hiv_status\", \"gwashiv\", \"hivstat\", \"hiv\"]\n",
    "#for match_var in match_list:\n",
    "\n",
    "# create the header for the output file\n",
    "out_head = \"{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\"genotype_id\", \"phenotype_column\", \"ancestry_selfreport\",\n",
    "                                           match_var.strip('\\\"'), \"age\", \"sex_selfreport\")\n",
    "gen = \"{}/processing/master.genotype.ids.n3469\".format(base_dir)\n",
    "phen = \"{}/unprocessed/hiv_all_merged_with_uhs_all_phenotype_data_08282017.csv\".format(base_dir)\n",
    "out_file = \"{}/processing/{}.genotype.to.phenotype.ancestry.{}.ha_{}percent.map\".format(base_dir,  \n",
    "                                                                                        date, match_var.strip('\\\"'), ha_percent)\n",
    "ha_ids = \"ha.ids.{}\".format(ha_percent)\n",
    "\n",
    "# view head of dictionaries\n",
    "def glance(d, size):\n",
    "    return dict(itertools.islice(d.items(), size))\n",
    "def gen_to_phen(match_var):\n",
    "    with open(gen) as asF, open(phen) as pF:\n",
    "        phead = pF.readline()\n",
    "        phead = phead.split(\",\")\n",
    "        \n",
    "        # note that all the cells in the phenotype file have quotes around the entries\n",
    "        # which is why we have to use double quotes for the following vars of interest\n",
    "        serum_index = phead.index('\"serum\"')\n",
    "        cell_line_index = phead.index('\"cell_line\"')\n",
    "        gwas_index = phead.index('\"gwasserum\"')\n",
    "        ancestry_index = phead.index('\"ancestry_selfreport\"')\n",
    "        age = phead.index('\"age\"')\n",
    "        sex = phead.index('\"sex_selfreport\"')\n",
    "        hiv_index = phead.index(match_var) \n",
    "\n",
    "        # initialized dictionaries that capture the variables-of-interest information \n",
    "        # for all subjects in phenotype file\n",
    "        cell_dic = {}\n",
    "        serum_dic = {}\n",
    "        gwas_dic = {}\n",
    "        \n",
    "        line = pF.readline() \n",
    "        while line: # parse each line of the master phenotype file\n",
    "            sl = line.split(\",\")\n",
    "            \n",
    "            # creating three mapping dictionaries because we are ultimately not sure\n",
    "            # which ID variable we will have to use to map the genotype ID to the corresponding\n",
    "            # phenotype information. It is actually going to take a combination of all three.\n",
    "            cell_dic[sl[cell_line_index]] = (phead[cell_line_index], sl[ancestry_index],\n",
    "                                             sl[hiv_index], sl[age], sl[sex])\n",
    "            serum_dic[sl[serum_index]] = (phead[serum_index], sl[ancestry_index], \n",
    "                                          sl[hiv_index], sl[age], sl[sex])\n",
    "            gwas_dic[sl[gwas_index]] = (phead[gwas_index], sl[ancestry_index],\n",
    "                                        sl[hiv_index], sl[age], sl[sex])\n",
    "            line = pF.readline()\n",
    "        # view dictionary \n",
    "        # print(glance(cell_dic, 3))\n",
    "\n",
    "        keep_list = []\n",
    "        head = asF.readline()\n",
    "        sline = asF.readline()\n",
    "        while sline:\n",
    "            spl = sline.split()\n",
    "            spl = [f'\"{word}\"' for word in spl]\n",
    "            if spl[1] in cell_dic:\n",
    "                tmptup = (spl[2], cell_dic[spl[1]])\n",
    "                keep_list.append(tmptup)\n",
    "            elif spl[0] in serum_dic:\n",
    "                tmptup = (spl[2], serum_dic[spl[0]])\n",
    "                keep_list.append(tmptup)\n",
    "            elif spl[0] in gwas_dic:\n",
    "                tmptup = (spl[2], gwas_dic[spl[0]])\n",
    "                keep_list.append(tmptup)\n",
    "            else:\n",
    "                print(spl[2])\n",
    "            sline = asF.readline()\n",
    "\n",
    "\n",
    "        print(len(keep_list))\n",
    "        mytup = keep_list[1]\n",
    "        mytup = (mytup[0],) + mytup[1]\n",
    "\n",
    "        mapped_ids = [(x[0],) + x[1] for x in keep_list]\n",
    "        print(mapped_ids[:5])\n",
    "\n",
    "\n",
    "        with open(out_file, 'w') as outF:\n",
    "            outF.write(out_head + \"\\n\")\n",
    "            for x in mapped_ids:\n",
    "                line = \"\\t\".join(str(i).strip('\\\"') for i in x)\n",
    "                outF.write(line + \"\\n\")\n",
    "            print(\"done\")\n",
    "            print(match_var)\n",
    "gen_to_phen(match_var)\n",
    "\n",
    "#    def ha_filter(ha_ids, map_file):\n",
    "#        out_file2 = \"{}.ha_only\".format(map_file)\n",
    "#        with open(ha_ids) as inF, open(map_file) as mF, open(out_file2, \"w\") as outF:\n",
    "#            head = mF.readline()\n",
    "#            outF.write(head)\n",
    "#            data_dic = {}\n",
    "#            line = mF.readline()\n",
    "#            while line:\n",
    "#                sl = line.split()\n",
    "#                data_dic[sl[0]] = line\n",
    "#                line = mF.readline()\n",
    "#\n",
    "#            line = inF.readline()\n",
    "#            while line:\n",
    "#                sl = line.strip()\n",
    "#                outF.write(data_dic[sl])\n",
    "#                line = inF.readline()\n",
    "#\n",
    "    #    ha_filter(ha_ids, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"HHG4618\"': ('\"cell_line\"', '\"2\"', '\"-9\"', '\"43\"', '\"1\"'), '\"HHG6025\"': ('\"cell_line\"', '\"1\"', '\"-9\"', '\"36\"', '\"1\"'), '\"HHG0254\"': ('\"cell_line\"', '\"2\"', '\"-9\"', '\"55\"', '\"1\"')}\n",
      "3469\n",
      "[('\"AS00-00347_8002022294_HHG10078_12_H06\"', '\"cell_line\"', '\"3\"', '\"-9\"', '\"49\"', '\"1\"'), ('\"AS00-00351_8002220319_HHG6146_36_C02\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"35\"', '\"2\"'), ('\"AS00-00437_8002220343_HHG6150_36_D02\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"41\"', '\"1\"'), ('\"AS00-00458_8002694957_HHG0612_1_D01\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"50\"', '\"1\"'), ('\"AS00-00459_8002220355_HHG6152_36_E02\"', '\"cell_line\"', '\"2\"', '\"-9\"', '\"52\"', '\"2\"')]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "### python ###\n",
    "import itertools, os\n",
    "\"\"\"\n",
    "This function will parse the master phenotype file and map the genotype IDs to the\n",
    "corresponding phenotype IDs. This was developed because there is no straight forward \n",
    "way to map the gentype IDs to the phenotype file. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "base_dir = \"/Users/jmarks/OneDrive - Research Triangle Institute/Projects/heroin/ngc/uhs4/phenotype\"\n",
    "date = \"20190320\"\n",
    "ha_percent = \"8\"\n",
    "match_var = '\"viralload_log10.y\"' # phenotype variable of interest in the master phenotype file\n",
    "#match_list = [\"viralload_cperml.y\", \"viralload_cperml.x\", \"hiv_status\", \"gwashiv\", \"hivstat\", \"hiv\"]\n",
    "#for match_var in match_list:\n",
    "\n",
    "# file which contains only the subject genotype IDs of the subjects that were classified\n",
    "# as HA after the STRUCTURE analysis\n",
    "ha_ids = \"{}/processing/ha.ids.{}\".format(base_dir, ha_percent) \n",
    "\n",
    "# create the header for the output file\n",
    "out_head = \"{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(\"genotype_id\", \"phenotype_column\", \"ancestry_selfreport\",\n",
    "                                           match_var.strip('\\\"'), \"age\", \"sex_selfreport\")\n",
    "gen = \"{}/processing/master.genotype.ids.n3469\".format(base_dir)\n",
    "phen = \"{}/unprocessed/hiv_all_merged_with_uhs_all_phenotype_data_08282017.csv\".format(base_dir)\n",
    "out_file = \"{}/processing/{}.genotype.to.phenotype.ancestry.{}.ha_{}percent.map\".format(base_dir,  \n",
    "                                                                                        date, match_var.strip('\\\"'), ha_percent)\n",
    "\n",
    "# view head of dictionaries\n",
    "def glance(d, size):\n",
    "    return dict(itertools.islice(d.items(), size))\n",
    "def gen_to_phen(match_var):\n",
    "    with open(gen) as asF, open(phen) as pF:\n",
    "        phead = pF.readline()\n",
    "        phead = phead.split(\",\")\n",
    "        \n",
    "        # note that all the cells in the phenotype file have quotes around the entries\n",
    "        # which is why we have to use double quotes for the following vars of interest\n",
    "        serum_index = phead.index('\"serum\"')\n",
    "        cell_line_index = phead.index('\"cell_line\"')\n",
    "        gwas_index = phead.index('\"gwasserum\"')\n",
    "        ancestry_index = phead.index('\"ancestry_selfreport\"')\n",
    "        age = phead.index('\"age\"')\n",
    "        sex = phead.index('\"sex_selfreport\"')\n",
    "        hiv_index = phead.index(match_var) \n",
    "\n",
    "        # initialized dictionaries that capture the variables-of-interest information \n",
    "        # for all subjects in phenotype file\n",
    "        cell_dic = {}\n",
    "        serum_dic = {}\n",
    "        gwas_dic = {}\n",
    "        \n",
    "        line = pF.readline() \n",
    "        while line: # parse each line of the master phenotype file\n",
    "            sl = line.split(\",\")\n",
    "            \n",
    "            # creating three mapping dictionaries because we are ultimately not sure\n",
    "            # which ID variable we will have to use to map the genotype ID to the corresponding\n",
    "            # phenotype information. It is actually going to take a combination of all three.\n",
    "            cell_dic[sl[cell_line_index]] = (phead[cell_line_index], sl[ancestry_index],\n",
    "                                             sl[hiv_index], sl[age], sl[sex])\n",
    "            serum_dic[sl[serum_index]] = (phead[serum_index], sl[ancestry_index], \n",
    "                                          sl[hiv_index], sl[age], sl[sex])\n",
    "            gwas_dic[sl[gwas_index]] = (phead[gwas_index], sl[ancestry_index],\n",
    "                                        sl[hiv_index], sl[age], sl[sex])\n",
    "            line = pF.readline()\n",
    "            # \n",
    "        print(glance(cell_dic, 3))\n",
    "\n",
    "        keep_list = []\n",
    "        next(asF) # skip header line\n",
    "        sline = asF.readline()\n",
    "        while sline:\n",
    "            spl = sline.split()\n",
    "            spl = [f'\"{word}\"' for word in spl]\n",
    "            if spl[1] in cell_dic:\n",
    "                tmptup = (spl[2], cell_dic[spl[1]])\n",
    "                keep_list.append(tmptup)\n",
    "            elif spl[0] in serum_dic:\n",
    "                tmptup = (spl[2], serum_dic[spl[0]])\n",
    "                keep_list.append(tmptup)\n",
    "            elif spl[0] in gwas_dic:\n",
    "                tmptup = (spl[2], gwas_dic[spl[0]])\n",
    "                keep_list.append(tmptup)\n",
    "            else:\n",
    "                print(spl[2])\n",
    "            sline = asF.readline()\n",
    "\n",
    "\n",
    "        print(len(keep_list))\n",
    "        mytup = keep_list[1]\n",
    "        mytup = (mytup[0],) + mytup[1]\n",
    "\n",
    "        mapped_ids = [(x[0],) + x[1] for x in keep_list]\n",
    "        print(mapped_ids[:5])\n",
    "\n",
    "\n",
    "        with open(out_file, 'w') as outF:\n",
    "            outF.write(out_head + \"\\n\")\n",
    "            for x in mapped_ids:\n",
    "                line = \"\\t\".join(str(i).strip('\\\"') for i in x)\n",
    "                outF.write(line + \"\\n\")\n",
    "            print(\"done\")\n",
    "gen_to_phen(match_var)\n",
    "\n",
    "\n",
    "# filter the map file that was created above to only the subjects classified\n",
    "# as HA after the STRUCTURE analysis, as well as .\n",
    "def ha_filter(ha_ids, map_file):\n",
    "    out_file2 = \"{}.ha_only\".format(map_file)\n",
    "    with open(ha_ids) as inF, open(map_file) as mF, open(out_file2, \"w\") as outF:\n",
    "        head = mF.readline()\n",
    "        outF.write(head)\n",
    "        data_dic = {}\n",
    "        line = mF.readline()\n",
    "        while line:\n",
    "            sl = line.split()\n",
    "            data_dic[sl[0]] = line\n",
    "            line = mF.readline()\n",
    "\n",
    "        line = inF.readline()\n",
    "        while line:\n",
    "            sl = line.strip()\n",
    "            outF.write(data_dic[sl])\n",
    "            line = inF.readline()\n",
    "\n",
    "ha_filter(ha_ids, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'viralload_cperml.y'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BASH ##\n",
    "awk '$4!=\"-9\"' 20190320.genotype.to.phenotype.ancestry.viralload_log10.y.ha_8percent.map.ha_only >\\\n",
    "    20190320.genotype.to.phenotype.ancestry.viralload_log10.y.ha_8percent.map.ha_only.complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
