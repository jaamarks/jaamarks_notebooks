{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fwGWAS - Naïve Method (Sveinbjornsson et al)\n",
    "**Notebook Author:** Jesse Marks <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Description\n",
    "This is the first method we are testing for the functional weighting GWAS (fwGWAS) method comparison. We are referring to this method as the naïve method, or the baseline model. This approach—based off of the 2016 Nature Genetics paper by [Sveinbjornsson et al](https://pubmed.ncbi.nlm.nih.gov/26854916/)—uses different P-value thresholds for each sequence variant functional category. More specifically, sequence variants are grouped into four categories: <br>\n",
    "i) **loss-of-function variants** <br>\n",
    "ii) **moderate-impact variants** <br>\n",
    "iii) **low-impact variants** <br>\n",
    "iv) **other** <br>\n",
    "\n",
    "A different P-value threshold will be applied to each of these four categories based off of their functional annotation and its putative functional effects. We use the software [SnpEff](https://pcingola.github.io/SnpEff/) to annotate the variants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We apply this method to 5 sets of GWAS results:\n",
    "* `s3://rti-heroin/fwGWAS/gwas1/bpd.pgc.2012-04.hg19lifted.txt`\n",
    "* `s3://rti-heroin/fwGWAS/gwas1/mpv1_2016_pval.txt`\n",
    "* `s3://rti-heroin/fwGWAS/gwas1/pgc.mdd.full.2012-04.hg19.phase3ID.txt`\n",
    "* `s3://rti-heroin/fwGWAS/gwas1/pgc.scz.full.2012-04.hg19.phase3ID.txt`\n",
    "* `s3://rti-heroin/fwGWAS/gwas1/wbc_astle2016.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook organization\n",
    "This notebook contains the analysis pipeline (Python3 code). We break down each function into a cell with a description of the data processes that's being carried out. The *main* function executes each function in serial. We lastly plot the results from the summary file of the naïve method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve method pipeline\n",
    "There are 7 Python3 steps/functions that comprise the pipeline. \n",
    "1. Convert in GWAS summary stats to VCF format.\n",
    "2. Run SnpEff to obtain the variant annotations.\n",
    "3. Extract relevant information from the variant annotations.\n",
    "4. Group the variants into their functional categories based off of their annotations.\n",
    "5. Apply the functionally weighted thresholds to the results.\n",
    "6. Apply the standard bonferroni correction threshold to the results.\n",
    "7. Create a summary.\n",
    "\n",
    "These 7 functions can be linked together to run automatically in serial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function1—Convert results to VCF format\n",
    "Converting GWAS Results to Variant Call Format ([VCF](https://en.wikipedia.org/wiki/Variant_Call_Format)) is the first step because the SnpEff software expects an input file in VCF format. \n",
    "\n",
    "The final VCF file should look like the following:\n",
    "```\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n",
    "22\t17675324\trs5748937\tT\tC\t.\t.\t0.7533\n",
    "22\t17798848\trs77501298\tC\tG\t.\t.\t0.646\n",
    "22\t17699299\trs5748957\tT\tG\t.\t.\t0.6269\n",
    "22\t17450765\trs61738794\tA\tG\t.\t.\t0.7285\n",
    "```\n",
    "\n",
    "The CHROM, POS, ID, REF, and ALT must be provided. The columns `QUAL` and `FILTER` in the VCF file will be left empty, or rather mapped to a period—this is the approach shown in the examples in the SnpEff manual.\n",
    "\n",
    "This function also creates the table to create the GWAS plot (Manhattan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function1\n",
    "def results_to_vcf_format(in_file, out_file, manhattan_table, pvalue, chromosome, snp, position, ref, alt):\n",
    "    \"\"\"\n",
    "    The SnpEff software expects, as input, a file in VCF format. \n",
    "    This function performs the conversion of the GWAS results \n",
    "    to VCF format so that SnpEff can obtain the annotations.\n",
    "    Also, generate the table needed for the plotting script that \n",
    "    creates the Manhattan GWAS plot. \n",
    "    \n",
    "    INPUT:\n",
    "    in_file - GWAS results file. \n",
    "    out_file - Name (and path) of the VCF file to be created.\n",
    "    \n",
    "    OUTPUT:\n",
    "    VCF file\n",
    "    GWAS plotting table to generate the Manhattan plots.\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "\n",
    "    try:\n",
    "        with gzip.open(in_file, 'rt') as inF:\n",
    "            with open(out_file, 'wt') as outF, open(manhattan_table, 'wt') as tableF:\n",
    "                line = inF.readline()\n",
    "\n",
    "                # for GWAS plotting table\n",
    "                table_header = ('VARIANT_ID', 'CHR', 'POSITION', 'P', 'TYPE')\n",
    "                table_header = '\\t'.join(table_header)\n",
    "                tableF.write(table_header + '\\n')\n",
    "    \n",
    "                # for VCF file\n",
    "                head_line = ['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO']\n",
    "                outF.write('\\t'.join(head_line) + '\\n')\n",
    "                split_line = line.split()\n",
    "     \n",
    "                pval_index = split_line.index(pvalue)\n",
    "                chr_index = split_line.index(chromosome)\n",
    "                rsid_index = split_line.index(snp)\n",
    "                position_index = split_line.index(position)\n",
    "                ref_allele_index = split_line.index(ref)\n",
    "                alt_allele_index = split_line.index(alt)\n",
    "\n",
    "                # skip the header now\n",
    "                line = inF.readline()\n",
    "                while(line):\n",
    "                    split_line = line.split()\n",
    "     \n",
    "                    # VCF\n",
    "                    f1 = split_line[chr_index]\n",
    "                    f2 = split_line[position_index]\n",
    "                    f3 = split_line[rsid_index]\n",
    "                    f4 = split_line[ref_allele_index]\n",
    "                    f5 = split_line[alt_allele_index]\n",
    "                    f6 = '.'\n",
    "                    f7 = '.'\n",
    "                    f8 = split_line[pval_index]\n",
    "     \n",
    "                    vcf_list = [f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "                    outF.write('\\t'.join(vcf_list) + '\\n')\n",
    "\n",
    "                    # GWAS table\n",
    "                    unique_id = '{}:{}:{}:{}'.format(f1,f2,f4,f5)\n",
    "\n",
    "                    acgt = ('A', 'C', 'G', 'T')\n",
    "                    if (f4 and f5) in acgt:\n",
    "                        var_type = 'snp'\n",
    "                    else:\n",
    "                        var_type = 'indel'\n",
    "\n",
    "                    line_values = (unique_id, f1, f2, f8, var_type)\n",
    "                    table_line = '\\t'.join(line_values) \n",
    "                    tableF.write(table_line + '\\n')\n",
    "\n",
    "                    line = inF.readline()\n",
    "\n",
    "    except (OSError):\n",
    "        print(\"Please gzip your input file.\")\n",
    "    return (out_file, manhattan_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function2—Obtain variant annotations with SnpEff \n",
    "\n",
    "**Note**: adjust the java memory specification as needed. Default allocation is 2GB; here we specified 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function2\n",
    "def snp_eff(base_dir, in_file, out_file, config, jar):\n",
    "    \"\"\"\n",
    "    This function executes the SnpEff software that annotates the\n",
    "    sequence variants using the Genome Build 37 as the reference.'\n",
    "    \n",
    "    INPUT:\n",
    "    base_dir - path were results should be saved.\n",
    "    in_file - name (and path) of VCF file for input to SnpEff\n",
    "    out_file - name of the output annotated VCF.\n",
    "    \n",
    "    OUTPUT:\n",
    "    File with variant annotations.\n",
    "    \"\"\"\n",
    "    \n",
    "    import subprocess\n",
    "\n",
    "    ### DO NOT modify these variables\n",
    "    ###########################################################################\n",
    "    config_path = config \n",
    "    snpEff_path = jar\n",
    "    snp_eff = base_dir + 'snp-eff.sh'\n",
    "    # -t for multithreading implies -noStats (speeds process way up)\n",
    "    command_list = ['java', '-Xmx8g', '-jar', snpEff_path, '-c', config_path, '-v',\n",
    "                     'GRCh37.75', in_file, '>', out_file]\n",
    "    command_string = ' '.join(command_list)\n",
    "    ###########################################################################\n",
    "\n",
    "    # save command as a bash script\n",
    "    with open(snp_eff, 'w') as outF:\n",
    "        message = '#!/usr/bin/bash\\n'\n",
    "        message += command_string\n",
    "        outF.write(message)\n",
    "\n",
    "    # execute bash script\n",
    "    run_command = ['bash', snp_eff]\n",
    "    subprocess.run(run_command)\n",
    "    return out_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function3—extract annotation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function3\n",
    "def extract_ann(in_file, out_file):\n",
    "    \"\"\"\n",
    "    Extract the annotation information from the SnpEff\n",
    "    results VCF file. The unique ID and GWAS P-value\n",
    "    for each variant is extracted as well.\n",
    "\n",
    "    INPUT:\n",
    "    in_file - The name of the file that was output from SnpEff.\n",
    "              The file should be in vcf format and have in the\n",
    "              INFO field the pval+annotation for each variant.\n",
    "    out_file - Name of the file for which to save the results\n",
    "               of this function to. This file will have the\n",
    "               following three fields:\n",
    "          1. unique ID (CHR:POSTION:A1:A2)\n",
    "          2. sequence variant annotation (e.g. stop-gain,)\n",
    "          3. P-value\n",
    "    OUTPUT:\n",
    "    File containing only the relevant information for our study from\n",
    "    The SnpEff output.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(in_file, 'r') as inF:\n",
    "        with open(out_file, 'w') as outF:\n",
    "            line = inF.readline()\n",
    "\n",
    "            while line[0] == '#':\n",
    "                line = inF.readline()\n",
    "     \n",
    "            while(line):\n",
    "                split_line = line.split()\n",
    "                unique_id = split_line[0] + ':' + split_line[1] + \\\n",
    "                    ':' + split_line[3] + ':' + split_line[4]\n",
    "                rsID = split_line[2]\n",
    "                info_field = split_line[7]\n",
    "                all_annotations = info_field.split(';')\n",
    "                pval = all_annotations[0]\n",
    "                functional_annotations = all_annotations[1].split(',')\n",
    "                for item in functional_annotations:\n",
    "                    output = rsID + '\\t' + unique_id + '\\t' + item.split('|')[1] + '\\t' + pval\n",
    "                    outF.write(output + '\\n')\n",
    "                line = inF.readline()  \n",
    "    return out_file;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function4—Group Variants into Four Annotational Categories\n",
    "\n",
    "Sequence variant annotation of each function group as described by the 2016 Nature paper by Sveinbjornsson et al.\n",
    "1. loss-of-function (stop-gain & stop-loss, frameshift indel, donor and acceptor splice-site, and initiator codon variants)\n",
    "2. moderate-impact (missense, in-frame indel and splice region variants)\n",
    "3. low-impact (synonymous, 3' and 5' UTR, and upstream and downstream variants)\n",
    "4. other (all other variants)\n",
    "\n",
    "**Note**: there are some variant annotations whose classification is in discordance when considering the Nature paper vs the SnpEff report on (putative) variant impact. For example, initiator codon variants are considered to be in the loss of function group according to the Nature paper where as the SnpEff software annotations the impact as low. There could be issues with this when considering *other* variants. One of these *other* variants might actually be a loss of impact variant or even a moderate impact. The threshold for *other* variants is set more stringent so that we might filter one of these variants out when in actuality we should have lower the threshold. This is just a side-note; I have not tried to determine if there are any instances of this yet. We should take a closer look at it though.\n",
    "\n",
    "### SnpEff annotation (Sequence Ontology terms)\n",
    "\n",
    "<br>\n",
    "\n",
    "**loss-of-function**\n",
    "* stop_gained \n",
    "* stop_lost\n",
    "* frameshift_variant\n",
    "* splice_donor_variant\n",
    "* splice_acceptor_variant\n",
    "* initiator_codon_variant (note that SnpEff indicates that the impact of these variants are LOW)\n",
    "\n",
    "<br>\n",
    "\n",
    "**moderate-impact-variants**\n",
    "* missense_variant\n",
    "* inframe_insertion\n",
    "* splice_region_variant (impact LOW)\n",
    "\n",
    "<br>\n",
    "\n",
    "**low-impact variants**\n",
    "* synonymous_variant\n",
    "* 3_prime_UTR_variant\n",
    "* 5_prime_UTR_variant\n",
    "* upstream_gene_variant\n",
    "* downstream_gene_variant\n",
    "\n",
    "**other**\n",
    "* All other variant annotations detailed in SnpEff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_annotations(in_file, out_file):\n",
    "    \"\"\"\n",
    "    Categorized the variants into groups based off\n",
    "    of their annotations.\n",
    "    \n",
    "    INPUT:\n",
    "    in_file - the file name (and path) of the file\n",
    "              that contains the variant annotations.\n",
    "    out_file - the file name (and path) of the output file\n",
    "               that will essentially append the variant group\n",
    "               to the in_put file.\n",
    "    \n",
    "    OUTPUT:\n",
    "    File with the SNPs grouped into the four functional categories.\n",
    "    \"\"\"\n",
    "    with open(in_file, 'r') as inF:\n",
    "        with open(out_file, 'w') as outF:\n",
    "            line = inF.readline()\n",
    "            group_dict = {}\n",
    "            # add loss-of-function variants\n",
    "            group_dict['loss-of-function variants'] = ['stop_gained', \n",
    "                                                      'stop_lost', \n",
    "                                                      'frameshift_variant', \n",
    "                                                      'splice_donor_variant', \n",
    "                                                      'splice_acceptor_variant', \n",
    "                                                      'initiator_codon_variant']\n",
    "            # add moderate-impact variants\n",
    "            group_dict['moderate-impact variants'] = ['missense_variant', \n",
    "                                                     'inframe_insertion', \n",
    "                                                     'splice_region_variant']\n",
    "            # add low-impact variants\n",
    "            group_dict['low-impact variants'] = ['synonymous_variant', \n",
    "                                                '3_prime_UTR_variant', \n",
    "                                                '5_prime_UTR_variant', \n",
    "                                                'upstream_gene_variant', \n",
    "                                                'downstream_gene_variant']\n",
    "            ## Group variants based on their categorization.\n",
    "            while(line):\n",
    "                split_line = line.split()\n",
    "                # Search for ann in dict.\n",
    "                for item in group_dict:\n",
    "                    if split_line[2] in group_dict[item]:\n",
    "                        split_line.append(item)\n",
    "                        break\n",
    "                # If the variant was not in any of the three groups;\n",
    "                # then it is categorized as an 'other' variant.\n",
    "                if len(split_line) == 4:\n",
    "                    split_line.append('other')\n",
    "                outF.write('\\t'.join(split_line) + '\\n')\n",
    "                line = inF.readline()\n",
    "    return out_file;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function5—Filter variants based off of the functional weighted Thresholds\n",
    "1. Loss of function; $5.5 \\times 10^{-7}$ \n",
    "2. Moderate impact; $1.1 \\times 10^{-7}$ \n",
    "3. Low impact; $1.0 \\times 10^{-8}$ \n",
    "4. Other; $1.7 \\times 10^{-9}$ \n",
    "\n",
    "These thresholds are based off of the estimated enrichment of categories among association signals of 1000G and the resulting significance thresholds detailed in 2016 Nature paper by Sveinbjornsson et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function5\n",
    "def filter_fw(in_file, out_file):\n",
    "    \"\"\"\n",
    "    This function filters the variants by comparing the sequence\n",
    "    variant P-values obtained from the GWAS results with the \n",
    "    functional-weighted threshold.\n",
    "    \n",
    "    INPUT:\n",
    "    in_file - name (and path) of the input file that contains the \n",
    "              variant groupings.\n",
    "    out_file - name (and path) of the output file that will be the\n",
    "               fw-threshold filtered file.\n",
    "    \n",
    "    OUTPUT:\n",
    "    A file containing functionally weighted significant variants.\n",
    "    \"\"\"\n",
    "    with open(in_file) as inF:\n",
    "\n",
    "        # keep track of duplicate variants\n",
    "        fw_dict = {}\n",
    "\n",
    "        with open(out_file, 'w') as outF:\n",
    "            line = inF.readline()\n",
    "            loss_func = 5.5e-7\n",
    "            mod_impact = 1.1e-7\n",
    "            low_impact = 1.0e-8\n",
    "            other = 1.7e-9\n",
    "\n",
    "            # map group to new threshold. also give each group a rank\n",
    "            # so that lower priority duplicated SNPs will be filtered out.\n",
    "            thresh_dict = {'loss-of-function variants': [loss_func, 4],\n",
    "                           'moderate-impact variants': [mod_impact,3],\n",
    "                           'low-impact variants': [low_impact,2],\n",
    "                           'other': [other,1]}\n",
    "            while(line):\n",
    "                split_line = line.split('\\t')\n",
    "                unique_id = split_line[1]\n",
    "                group = split_line[4].strip()\n",
    "                pval = float(split_line[3])\n",
    "                if pval <= thresh_dict[group][0]:\n",
    "                    if unique_id not in fw_dict:\n",
    "                        # store variant information and its rank\n",
    "                        fw_dict[unique_id] = [line, thresh_dict[group][1]]\n",
    "                    else:\n",
    "                        # if the rank of this duplicated SNP is higher than the that\n",
    "                        # of the previously recored rank then keep this new line\n",
    "                        if fw_dict[unique_id][1] < thresh_dict[group][1]:\n",
    "                            fw_dict[unique_id] = [line, thresh_dict[group][1]]\n",
    "\n",
    "                line = inF.readline()\n",
    "\n",
    "            for item in fw_dict:\n",
    "                line = fw_dict[item][0]\n",
    "                outF.write(line)\n",
    "                \n",
    "    return out_file;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Function6&mdash;Filter variants based off of the standard P-value\n",
    "Filter the sequence variants by the standard genome-wide significance (WGS) P-Value threshold of $5\\times 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function6\n",
    "def filter_standard(in_file, out_file):\n",
    "    \"\"\"\n",
    "    This function filters the variants by comparing the sequence\n",
    "    variant pvalues obtained from the GWAS results with the \n",
    "    standard GWAS threshold of 5e-8.\n",
    "         \n",
    "    INPUT:\n",
    "    in_file - name (and path) of the input file that contains the \n",
    "              variant groupings.\n",
    "    out_file - name (and path) of the output file that will be the\n",
    "               standard-threshold filtered file.\n",
    "    \n",
    "    OUTPUT:\n",
    "    A file containing significant variants based off of standard threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of duplicates\n",
    "    std_dict = {}\n",
    "\n",
    "    # give each group a rank so that lower priority duplicated SNPs will be filtered out.\n",
    "    thresh_dict = {'loss-of-function variants': [ 4],\n",
    "                   'moderate-impact variants': [3],\n",
    "                   'low-impact variants': [2],\n",
    "                   'other': [1]}\n",
    "\n",
    "    with open(in_file) as inF:\n",
    "        with open(out_file, 'w') as outF:\n",
    "            line = inF.readline()\n",
    "            bf_threshold = 5e-8\n",
    "            \n",
    "            while(line):\n",
    "                split_line = line.split('\\t')\n",
    "                unique_id = split_line[1]\n",
    "                group = split_line[4].strip()\n",
    "                pval = float(split_line[3])\n",
    "                if pval <= bf_threshold:\n",
    "                    if unique_id not in std_dict:\n",
    "                        # store variant information and its rank\n",
    "                        std_dict[unique_id] = [line, thresh_dict[group][0]]\n",
    "                    else:\n",
    "                        # if the rank of this duplicated SNP is higher than the that\n",
    "                        # of the previously recored rank then keep this new line\n",
    "                        if std_dict[unique_id][1] < thresh_dict[group][0]:\n",
    "                            std_dict[unique_id] = [line, thresh_dict[group][0]]\n",
    "\n",
    "                line = inF.readline()\n",
    "\n",
    "            for item in std_dict:\n",
    "                line = std_dict[item][0]\n",
    "                outF.write(line)\n",
    "    return out_file;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function7—Report Results\n",
    "Results comparison: compare the sequence variants that were deemed significant when using the fw-thresholds vs the standard WGS P-value threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function7\n",
    "def results_summary(in_file, in_file2, out_file):\n",
    "    \"\"\"\n",
    "    The function takes as input the two results files from the threshold filtering\n",
    "    performed above and outputs the summary statistics. Specifically, the output file \n",
    "    will contain two counts-dictionaries. One dict will count the number of variants \n",
    "    in each functional group that was exclusive to the fw-thresholded sequence\n",
    "    variants. The other will be for the variants exclusive to the standard WGS P-value\n",
    "    thresholded results.\n",
    "    \n",
    "    INPUT:\n",
    "    in_file -  name of the fw-thresholded file\n",
    "    in_file2 - name of the standard thresholded file\n",
    "    out_file     - Name of the file to which the summary statistics will be saved.\n",
    "    \n",
    "    OUTPUT:\n",
    "    Nothing is returned from this function, however three files are output. One is\n",
    "    the out_file provided as input, and then two additional files are created.\n",
    "    \n",
    "    additional01 - File assumes the name <out_file>-fw-variants and includes a list of \n",
    "                   the variants that were deemed significant only when the fw-thresholds\n",
    "                   were applied as thresholds of significance.\n",
    "    additional02 - File assumes the name <out_file>-std-variants and includes a list of\n",
    "                   the variants that were deemed significant only when the standard\n",
    "                   threshold (5e-8) was applied.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    ## compare the two filtered files and print the variants \n",
    "    #  Exclusive to the fw-thresholded variants\n",
    "    bash_command = 'bash -c \"comm -23 <(sort {0}) <(sort {1})\"'.format(in_file, in_file2)\n",
    "    bash_commanda = 'bash -c \"comm -23 <(sort {0}) <(sort {1}) | cut -f2\"'.format(in_file, in_file2)\n",
    "    fw_exclusive = subprocess.run(bash_command, shell=True, stdout=subprocess.PIPE, encoding='utf-8').stdout\n",
    "    fw_exclusivea = subprocess.run(bash_commanda, shell=True, stdout=subprocess.PIPE, encoding='utf-8').stdout\n",
    "\n",
    "    # Exlusive to the standard thresholded variants (5e-8)\n",
    "    bash_command2 = 'bash -c \"comm -13 <(sort {0}) <(sort {1})\"'.format(in_file, in_file2)\n",
    "    bash_command2a = 'bash -c \"comm -13 <(sort {0}) <(sort {1})| cut -f2\"'.format(in_file, in_file2)\n",
    "    standard_exclusive = subprocess.run(bash_command2, shell=True, stdout=subprocess.PIPE, encoding='utf-8').stdout\n",
    "    standard_exclusivea = subprocess.run(bash_command2a, shell=True, stdout=subprocess.PIPE, encoding='utf-8').stdout\n",
    "\n",
    "    # Variants deemed siginificant using both thresholding methods\n",
    "    bash_command3 = 'bash -c \"comm -12 <(sort {0}) <(sort {1})\"'.format(in_file, in_file2)\n",
    "    bash_command3a = 'bash -c \"comm -12 <(sort {0}) <(sort {1})| cut -f2\"'.format(in_file, in_file2)\n",
    "    both_methods = subprocess.run(bash_command3, shell=True, stdout=subprocess.PIPE, encoding='utf-8').stdout\n",
    "    both_methodsa = subprocess.run(bash_command3a, shell=True, stdout=subprocess.PIPE, encoding='utf-8').stdout\n",
    "\n",
    "    with open(out_file, 'w') as outF:\n",
    "        for count,string in enumerate([fw_exclusive, standard_exclusive, both_methods]):\n",
    "            counts_dict = {'loss-of-function variants':0,\n",
    "                           'moderate-impact variants':0,\n",
    "                           'low-impact variants':0,\n",
    "                           'other':0}\n",
    "            if count == 0:\n",
    "                message = '####\\n####Novel variants.\\n\\n'\n",
    "\n",
    "            elif count == 1:\n",
    "                message = '\\n\\n####\\n####Variants not replicated.\\n\\n'\n",
    "\n",
    "            else:\n",
    "               message = '\\n\\n####\\n####Variants that were replicated.\\n\\n'\n",
    "\n",
    "            split_lines = string.splitlines()\n",
    "            for line in split_lines:\n",
    "                ann = line.split('\\t')[4]\n",
    "                counts_dict[ann] += 1\n",
    "            outF.write(message)\n",
    "            dict_sum = str(sum(counts_dict.values()))\n",
    "            dict_sum = '\\nTotal number of elements: {}'.format(dict_sum)\n",
    "            outF.write(str(counts_dict) + dict_sum + '\\n')\n",
    "\n",
    "    novelfile = out_file+'-novel-variants'\n",
    "    with open(novelfile, 'w') as fwF:\n",
    "        fwF.write(fw_exclusivea)\n",
    "\n",
    "    lostfile = out_file+'-lost-variants'\n",
    "    with open(lostfile, 'w') as stdF:\n",
    "        stdF.write(standard_exclusivea)\n",
    "\n",
    "    dupfile = out_file+'-duplicated-variants'\n",
    "    with open(dupfile, 'w') as bothF:\n",
    "        bothF.write(both_methodsa)\n",
    "\n",
    "    return novelfile, lostfile, dupfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 8—Manhattan Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 8 \n",
    "def gwas_plot(out_dir, out_file, table, highlightlist=\"\", color=\"\"):\n",
    "    \"\"\"\n",
    "    Create Manhattan GWAS plot.\n",
    "    If you pass a highlight list along with the color green, red, or yellow,\n",
    "    then you will get variants highlighted in that color.\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "\n",
    "    if color == 'green':\n",
    "        plot_script = out_dir + 'plot_script_novel_variants.sh'\n",
    "        out_file = out_file[:-3] + '.gwas_plots_novel_variants'\n",
    "    elif color == 'red':\n",
    "        plot_script = out_dir + 'plot_script_lost_variants.sh'\n",
    "        out_file = out_file[:-3] + '.gwas_plots_lost_variants'\n",
    "    elif color == 'yellow':\n",
    "        plot_script = out_dir + 'plot_script_duplicated_variants.sh'\n",
    "        out_file = out_file[:-3] + '.gwas_plots_duplicated_variants'\n",
    "    else:\n",
    "        plot_script = out_dir + 'plot_script.sh'\n",
    "        out_file = out_file[:-3] + '.gwas_plots'\n",
    "        \n",
    "\n",
    "\n",
    "    # base plot without highlights\n",
    "    command_list = ['Rscript', '/home/ec2-user/bin/generate_gwas_plots.R',\n",
    "                     '--in', table,\n",
    "                     '--in_chromosomes autosomal_nonPAR',\n",
    "                     '--in_header',\n",
    "                     '--out', out_file, \n",
    "                     '--col_id VARIANT_ID',\n",
    "                     '--col_chromosome CHR',\n",
    "                     '--col_position POSITION',\n",
    "                     '--col_p P',\n",
    "                     '--col_variant_type TYPE ',\n",
    "                     '--generate_snp_indel_manhattan_plot',\n",
    "                     '--manhattan_odd_chr_color gray74',\n",
    "                     '--manhattan_even_chr_color gray87',\n",
    "                     '--manhattan_points_cex 1.5']\n",
    "\n",
    "    # if we need to highlight\n",
    "    if color != \"\":\n",
    "        command_list.extend(['--manhattan_highlight_color', color])\n",
    "        command_list.extend(['--highlight_list', highlightlist])\n",
    "                                          \n",
    "\n",
    "    command_string = ' '.join(command_list)\n",
    "\n",
    "    with open(plot_script, 'w') as outF:\n",
    "        message = '#!/usr/bin/bash\\n'\n",
    "        message += command_string\n",
    "        outF.write(message)\n",
    "\n",
    "    # execute bash script\n",
    "    run_command = ['bash', plot_script]\n",
    "    subprocess.run(run_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function\n",
    "\n",
    "Execute all functions in sequence. The user needs to only modify the four variables at the top of this main function; everything else is taken care of in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python3 ###\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Carryout all the functions necessary to \n",
    "    perform the fwGWAS naive method. Please there are simply for variables to\n",
    "    \"\"\" \n",
    "    \n",
    "    ### Variables to alter\n",
    "    ###############################################################################################\n",
    "\n",
    "    # config file (full path)\n",
    "    config=\"/.../snpEff/snpEff.config\"\n",
    "    # jar file (full path)\n",
    "    jar=\"/.../snpEff/snpEff.jar\"\n",
    "\n",
    "    # path to directory containing the GWAS results (should end with a forward # slash)\n",
    "    gwas_dir = ''\n",
    "\n",
    "    # name of the GWAS results file in the directory above (gzip results, extension .gz)\n",
    "    gwas_file= '.gz'\n",
    "\n",
    "    # path to directory that the processed data should be placed in (should end with a forward # slash)\n",
    "    out_dir = ''\n",
    "\n",
    "    snp = 'SNP' # name of variant id in the header\n",
    "    chrom = 'CHR' # name of chromosome in header\n",
    "    position = 'POS' # name of position in header\n",
    "    ref = 'A1' # name of reference allele\n",
    "    alt = 'A2' # name of alternative allele\n",
    "    pvalue= 'P' # name of pvalue in header\n",
    "\n",
    "\n",
    "    ### DO NOT alter below this line\n",
    "    ###############################################################################################\n",
    "    rtvf_in = gwas_dir + gwas_file\n",
    "    rtvf_out = out_dir + gwas_file[:-3] + '.vcf'\n",
    "    manhattan_table = out_dir + gwas_file[:-3] + '.plot_table'\n",
    "\n",
    "\n",
    "    # Convert GWAS results to VCF format\n",
    "    se_inF, plot_table = results_to_vcf_format(in_file=rtvf_in, \n",
    "            out_file=rtvf_out,\n",
    "            manhattan_table=manhattan_table,\n",
    "            pvalue=pvalue,\n",
    "            chromosome=chrom,\n",
    "            snp=snp,\n",
    "            position=position,\n",
    "            ref=ref,\n",
    "            alt=alt)\n",
    "\n",
    "    se_outF = se_inF[:-4] + '.annotated.vcf'\n",
    "\n",
    "    ## Run SnpEff to obtain annotations\n",
    "    ex_inF = snp_eff(base_dir=out_dir, \n",
    "                     in_file=se_inF, \n",
    "                     out_file=se_outF,\n",
    "                     config=config,\n",
    "                     jar=jar)\n",
    "\n",
    "    ex_outF = ex_inF[:-4] + '-cleaned'\n",
    "\n",
    "    ## Extract annotations, IDs, and P-values\n",
    "    ga_inF = extract_ann(in_file=ex_inF,\n",
    "                         out_file=ex_outF)\n",
    "    ga_outF = ga_inF + '-grouped'\n",
    "    \n",
    "\n",
    "    ## Group variants into the four function annotation categories\n",
    "    ff_inF = group_annotations(in_file=ga_inF,\n",
    "                               out_file=ga_outF)\n",
    "\n",
    "    #ff_inF = out_dir + 'test.txt-ann-cleaned-grouped'\n",
    "    ff_outF = ff_inF + '-filtered-fw'\n",
    "    \n",
    "\n",
    "    ## Filter using the four fw-thresholds\n",
    "    rs_inF1 = filter_fw(in_file=ff_inF,\n",
    "                        out_file=ff_outF)\n",
    "    \n",
    "    fs_outF = ff_inF + '-filtered-std'\n",
    "   \n",
    "\n",
    "    ## Filter using the standard GWAS threshold (5e-8)\n",
    "    rs_inF2 = filter_standard(in_file=ga_outF, \n",
    "                             out_file=fs_outF)\n",
    "\n",
    "    rs_out = rtvf_out[:-4] + '.results-summary'\n",
    "    \n",
    "    ## Get results summary\n",
    "    novel_vars, lost_vars, duplicated_vars = results_summary(in_file=rs_inF1, in_file2=rs_inF2, out_file=rs_out)\n",
    "\n",
    "\n",
    "    ## plot for highlighting novel, list, and duplicated variants\n",
    "    for count, highlightlist in enumerate([novel_vars, lost_vars, duplicated_vars]):\n",
    "        if count == 0: \n",
    "            color = 'green'\n",
    "        if count == 1: \n",
    "            color = 'red'\n",
    "        if count == 2: \n",
    "            color = 'yellow'\n",
    "\n",
    "        gwas_plot(out_dir=out_dir, out_file=gwas_file, table=plot_table, highlightlist=highlightlist, color=color)\n",
    "\n",
    "    ## plot for base Manhattan without highlights\n",
    "    gwas_plot(out_dir=out_dir, out_file=gwas_file, table=plot_table)\n",
    "\n",
    "############################################################################################################################\n",
    "# All 8 functions go here.\n",
    "\n",
    "\n",
    "# End of function definitions\n",
    "################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
