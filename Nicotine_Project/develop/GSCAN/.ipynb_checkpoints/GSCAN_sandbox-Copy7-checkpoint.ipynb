{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data and visualize\n",
    "Grab the beta value and the standard error (beta) for each SNP accross each cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 044, 045 and 046 meta-analysis calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from decimal import Decimal\n",
    "from scipy.stats import chi2\n",
    "import csv\n",
    "\n",
    "\"\"\"\n",
    "This function takes as input (1) a list of cohorts specific to a meta-analysis,\n",
    "(2) and an excel file containing the data on each variant that was the results \n",
    "of a SNP look-up. You need to specify the name of the sheet as well. This file should contain a header. For each column, the heading should be\n",
    "the cohort name & ancestry followed by a period followed by the data description. Specifically,\n",
    "the data that are of interest for this script are: the beta value, standard error, and the variant\n",
    "specific p-value. An example of how these entries should be in the excel sheet is\n",
    "\n",
    "Example:\n",
    "AAND_COGEND2_AA.beta_SNP_add,  AAND_COGEND2_AA.sebeta_SNP_add, AAND_COGEND2_AA.p\n",
    "\n",
    "For (1) an example of the input list is: \n",
    "AAND_COGEND2_AA, DECODE_EA, NONGAIN_EA \n",
    "\n",
    "The output will be an excel file with the meta-analysis calculations for each variant of interest.\n",
    "\"\"\"\n",
    "# I will create a new data frame for each variant. This data frame will have the column names:\n",
    "# (1) cohort, (2) Ancestry group, (3) Beta, (4) Std. Error, (5) Seweighted, and (6) Pr(>|t|) which is the p-val\n",
    "# addictionally  I will add the rows for calculating the meta in the Seweighted column and below all of the cohorts\n",
    "# getting the first cohort, note that this will eventually be in a loop of the cohorts\n",
    "\n",
    "#os.chdir(\"C:\\\\Users\\\\jmarks\\\\Desktop\\\\Projects\\\\Nicotine\\\\GSCAN_extended_results_nicotine\\\\results\\\\results_from_missing_snp_lookup\\\\\")\n",
    "#mypath = r\"C:\\Users\\jmarks\\Desktop\\Projects\\Nicotine\\GSCAN_extended_results_nicotine\\develop\\missing_snps_from_first_set_of_results\\results\\SNP-lookup-results\"\n",
    "#os.chdir(mypath)\n",
    "#mydata = pd.ExcelFile(\"missing_SNPs_results_prefiltered_meta_analyses_044_045_046_V02.xlsx\")\n",
    "#mydata = mydata.parse(\"044_combined_data\")\n",
    "\n",
    "mypath = r\"C:\\Users\\jmarks\\Desktop\\Projects\\Nicotine\\GSCAN_extended_results_nicotine\\develop\\missing_snps_from_first_set_of_results\\results\\SNP\" \n",
    "os.chdir(mypath)\n",
    "allMydata = pd.ExcelFile(\"mrBig.xlsx\")\n",
    "\n",
    "allCohorts = [\"AAND_COGEND2_AA\",\n",
    "    \"COGEND_AA\",\n",
    "    \"COGEND_EA\",\n",
    "    \"COGEND2_AA\",\n",
    "    \"COGEND2_EA\",\n",
    "    \"COPDGENE_AA\",\n",
    "    \"COPDGene_EA\",\n",
    "    \"deCODE_EA\",\n",
    "    \"Dental_Caries_EA\",\n",
    "    \"EAGLE_EA\",\n",
    "    \"FINN_TWIN_EA\",\n",
    "    \"GAIN_AA\",\n",
    "    \"GAIN_EA\",\n",
    "    \"JHS_AA\",\n",
    "    \"nonGAIN_EA\",\n",
    "    \"NTR_EA\",\n",
    "    \"SAGE_AA\",\n",
    "    \"SAGE_EA\",\n",
    "    \"UW_TTURC_AA\",\n",
    "    \"UW_TTURC_EA\",\n",
    "    \"YALE_PENN_AA\",\n",
    "    \"YALE_PENN_EA\"]\n",
    "\n",
    "count = 0 \n",
    "\n",
    "excelSheets = allMydata.sheet_names\n",
    "for sheet in excelSheets:\n",
    "    mydata = allMydata.parse(sheet)\n",
    "\n",
    "    cohorts_list = []\n",
    "    if count == 1:\n",
    "        for item in allCohorts:\n",
    "            if item[-2:]==\"EA\":\n",
    "                cohorts_list.append(item)\n",
    "    elif count == 2:\n",
    "        for item in allCohorts:\n",
    "            if item[-2:] == \"AA\":\n",
    "                cohorts_list.append(item)\n",
    "                \n",
    "                \n",
    "    else:\n",
    "        cohorts_list = allCohorts\n",
    "        \n",
    "    totalRows = len(cohorts_list)\n",
    "\n",
    "    # This dictionary will have an rsID for the key and the key value will be a dataframe\n",
    "    dataDict = {}\n",
    "\n",
    "    # initialize a dataframe\n",
    "    emptyArray = np.empty((totalRows,13,))\n",
    "    emptyArray[:] = np.nan\n",
    "    columns = [\"SNP\", \"Cohort\", \"Ancestry group\", \"Beta\", \"Std. Error\", \"Seweighted\", \"Pr(>|t|)\", \n",
    "               \"AllMeta.SumSEweight\", \"AllMeta.weightedSE\", \"AllMeta.SEweighted_beta\", \n",
    "               \"AllMeta.SEweighted_Z\", \"AllMeta.SEweighted_Chi\", \"AllMeta.SEweighted_P\"]\n",
    "    num_of_rsIDs = len(mydata)\n",
    "    # Above this write a script which removes the variants who were not present in any of the cohorts\n",
    "\n",
    "\n",
    "\n",
    "    # list of SNPs which were all NA across all cohorts\n",
    "    noDataSNPs = []\n",
    "    \n",
    "\n",
    "    # loop to fill in information for the meta-anlaysis calculation\n",
    "    for rsID in range(num_of_rsIDs):\n",
    "\n",
    "        # check SNP missing across all cohorts\n",
    "        if not pd.isnull(mydata.iloc[rsID,3:]).all():\n",
    "\n",
    "            markerName = mydata.iloc[rsID,0]\n",
    "            dataDict[markerName] = pd.DataFrame(columns=columns, data=emptyArray)\n",
    "            dataDict[markerName].iloc[0,0] = markerName # add SNP\n",
    "            metaSEweighted_beta = 0\n",
    "\n",
    "\n",
    "            for cohort in range(len(cohorts_list)):\n",
    "\n",
    "                # get all of the cohort specific data\n",
    "                cohortData = mydata.filter(like=cohorts_list[cohort]).iloc[rsID,:]\n",
    "\n",
    "                # add cohort to dataframe\n",
    "                cohortName = cohorts_list[cohort]\n",
    "                dataDict[markerName].iloc[cohort,1] = cohorts_list[cohort][0:-3]\n",
    "\n",
    "                # add Ancestry group\n",
    "                ancestry = cohorts_list[cohort][-2:]\n",
    "                dataDict[markerName].iloc[cohort, 2] = ancestry\n",
    "\n",
    "                # add Beta\n",
    "                betaVal = cohortData.filter(like=\".beta\")[0]\n",
    "\n",
    "                # flip the sign for deCODE and NTR\n",
    "                if cohorts_list[cohort] == \"FINN_TWIN_EA\":\n",
    "                    betaVal = -betaVal\n",
    "                dataDict[markerName].iloc[cohort, 3] = betaVal\n",
    "\n",
    "                # add Std. Error\n",
    "                standardErr = cohortData.filter(like=\"sebeta\")[0]\n",
    "                dataDict[markerName].iloc[cohort, 4] = standardErr\n",
    "\n",
    "                # add Seweighted\n",
    "                seWeighted = 1 / (standardErr ** 2)\n",
    "                dataDict[markerName].iloc[cohort,5]  = seWeighted\n",
    "\n",
    "                # add p-val\n",
    "                pVal = cohortData.filter(regex=\".p$\")[0]\n",
    "                dataDict[markerName].iloc[cohort, 6] = pVal\n",
    "\n",
    "                #  metaSEweighted_beta calculation \n",
    "                if not np.isnan(betaVal):\n",
    "                    metaSEweighted_beta += (betaVal*seWeighted)\n",
    "\n",
    "            # Meta calculations\n",
    "            SumSEweight = dataDict[markerName]['Seweighted'].sum()\n",
    "            dataDict[markerName].iloc[0, 7] = SumSEweight\n",
    "\n",
    "            metaWeightedSE = math.sqrt(1/SumSEweight)\n",
    "            dataDict[markerName].iloc[0, 8] = metaWeightedSE\n",
    "\n",
    "            metaSEweighted_beta = metaSEweighted_beta / SumSEweight \n",
    "            dataDict[markerName].iloc[0, 9] = metaSEweighted_beta\n",
    "\n",
    "            metaSEweighted_Z = (metaSEweighted_beta / metaWeightedSE)\n",
    "            dataDict[markerName].iloc[0, 10] = metaSEweighted_Z\n",
    "\n",
    "            metaSEweighted_chi = metaSEweighted_Z ** 2\n",
    "            dataDict[markerName].iloc[0, 11] = metaSEweighted_chi\n",
    "\n",
    "            metaSEweighted_P = '%.2E' % Decimal(chi2.sf(metaSEweighted_chi, 1))\n",
    "            dataDict[markerName].iloc[0, 12] = metaSEweighted_P\n",
    "\n",
    "        else:\n",
    "            noDataSNPs.append(mydata.iloc[rsID,0])\n",
    "            \n",
    "    count += 1\n",
    "\n",
    "\n",
    "\n",
    "    myfile1 = mypath + \"\\\\meta_analysis_calculation_results\\\\\" + sheet + \"_meta_results_for_missing_snps\"\n",
    "    # write meta-calculations for all variants to a file,\n",
    "    with open(myfile1, 'a') as outfile:\n",
    "        for item in dataDict:\n",
    "            dataDict[item].to_csv(outfile, sep='\\t', index=False)\n",
    "            outfile.write('\\n\\n\\n')\n",
    "\n",
    "\n",
    "    myfile2 = mypath + \"\\\\meta_analysis_calculation_results\\\\\" + sheet + \"_completely_missing_SNPs\"\n",
    "    # write the missing the missing SNPs to a file\n",
    "    with open(myfile2, 'w') as outfile:\n",
    "        for item in noDataSNPs:\n",
    "            outfile.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
