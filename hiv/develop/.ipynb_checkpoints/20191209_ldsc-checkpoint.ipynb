{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGC LDSC Regression\n",
    "**Author**: Jesse Marks <br>\n",
    "**GitHub Issue:** [#140](https://github.com/RTIInternational/bioinformatics/issues/140) <br>\n",
    "**Results Location:** \n",
    "\n",
    "Compare FOU, OAall, and OAexp to the following:\n",
    "* **Each other**\n",
    "* **FTND**\n",
    "* **Cigarettes per day (GSCAN)**\n",
    "* **Smoking cessation (current vs. former, GSCAN)**\n",
    "* **Smoking initiation (ever vs. never, GSCAN)**\n",
    "* **Age of smoking initiation (GSCAN)**\n",
    "* **Alcohol dependence**\n",
    "* **Alcohol drinks per week (GSCAN)**\n",
    "* *MVP alcohol*\n",
    "* *AUDIT*\n",
    "* **Cannabis use disorder**\n",
    "* **Lifetime cannabis use (ever vs. never)**\n",
    "* Parkinson's disease\n",
    "* Amyotrophic lateral sclerosis\n",
    "* Alzheimers disease\n",
    "* Intelligence\n",
    "* Childhood IQ\n",
    "* College completion\n",
    "* Years of schooling\n",
    "* Neuroticism\n",
    "* Conscientiousness\n",
    "* Openness to experience\n",
    "* **Posttraumatic Stress Disorder**\n",
    "* Attention deficit hyperactivity disorder\n",
    "* Depressive symptoms\n",
    "* Major depressive disorder\n",
    "* Bipolar disorder\n",
    "* Psychiatric cross-disorder\n",
    "* Schizophrenia\n",
    "* Autism spectrum disorder\n",
    "* Anorexia Nervosa\n",
    "* Subjective well being\n",
    "* Putamen volume\n",
    "* Accumbens volume\n",
    "* Pallidum volume\n",
    "* Caudate volume\n",
    "* Thalamus volume\n",
    "* Hippocampus volume\n",
    "* Intracranial volume\n",
    "\n",
    "\\* bold are in-house; and italics we will acquire soon.\n",
    "\n",
    "**Note** that most of the in-house data are located on S3 at: `s3://rti-nd/LDSC`\n",
    "\n",
    "We are going to utilize the [LD score regression pipeline](https://github.com/RTIInternational/ld-regression-pipeline) that Alex Waldrop developed to perform LD score regression. \n",
    "\n",
    "## Data\n",
    "NGC summary stats results location:\n",
    "* **FOU**: `s3://rti-midas-data/studies/ngc/meta/087/processing/fou/alive+cats+cogend+start+uhs1-4+vidus+yale-penn.ea.fou.chr[1-22].maf_gt_0.01.rsq_gt_0.3.gz`<br><br>\n",
    "* **OAall**: `s3://rti-midas-data/studies/ngc/meta/089/processing/oaall/cats+coga+decode+kreek+odb+uhs+vidus+yale-penn.ea.chr[1-22].maf_gt_0.01.rsq_gt_0.3.gz`<br><br>\n",
    "* **OAexp**: `s3://rti-midas-data/studies/ngc/meta/060/processing/oaexp/coga+decode+yale_penn.ea.chr[1-22].maf_gt_0.01.rsq_gt_0.3.gz`\n",
    "\n",
    "<br>\n",
    "\n",
    "**sample sizes**\n",
    "* OAall: 304507\n",
    "* OAexp: 5561\n",
    "* FOU: 5388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling\n",
    "Format the summary stats for input into cromwell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOU\n",
    "cd /shared/jmarks/heroin/ldsc/ngc_all/fou/001/processing\n",
    "for chr in {1..22};do \n",
    "    aws s3 cp s3://rti-midas-data/studies/ngc/meta/087/processing/fou/alive+cats+cogend+start+uhs1-4+vidus+yale-penn.ea.fou.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz . --quiet &\n",
    "done\n",
    "\n",
    "outf=fou_087.txt\n",
    "for chr in {1..22};do\n",
    "    inf=alive+cats+cogend+start+uhs1-4+vidus+yale-penn.ea.fou.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz\n",
    "    awk '{print $1,$2,$3,$4,$5,$6,$8}' <(zcat $inf) >> $outf\n",
    "done &\n",
    "\n",
    "gzip $outf\n",
    "## upload to S3\n",
    "aws s3 cp $outf.gz s3://rti-nd/LDSC/opioid_fou/$outf.gz\n",
    "    \n",
    "    \n",
    "\n",
    "## OAexp\n",
    "for chr in {1..22}; do\n",
    "    aws s3 cp s3://rti-midas-data/studies/ngc/meta/060/processing/oaexp/coga+decode+yale_penn.ea.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz . --quiet &\n",
    "done\n",
    "\n",
    "outf=oaexp_060.txt\n",
    "for chr in {1..22};do\n",
    "    inf=coga+decode+yale_penn.ea.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz\n",
    "    awk '{print $1,$2,$3,$4,$5,$6,$8}' <(zcat $inf) >> $outf\n",
    "done &\n",
    "\n",
    "gzip $outf\n",
    "## upload to S3\n",
    "aws s3 cp $outf.gz s3://rti-nd/LDSC/opioid_oaexp/$outf.gz\n",
    "\n",
    "\n",
    "## OAall\n",
    "for chr in {1..22}; do\n",
    "    aws s3 cp s3://rti-midas-data/studies/ngc/meta/089/processing/oaall/cats+coga+decode+kreek+odb+uhs+vidus+yale-penn.ea.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz . --quiet &\n",
    "done\n",
    "\n",
    "outf=oaall_089.txt\n",
    "for chr in {1..22};do\n",
    "    inf=cats+coga+decode+kreek+odb+uhs+vidus+yale-penn.ea.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz\n",
    "    awk '{print $1,$2,$3,$4,$5,$6,$8}' <(zcat $inf) >> $outf\n",
    "done &\n",
    "\n",
    "gzip $outf\n",
    "## upload to S3\n",
    "aws s3 cp $outf.gz s3://rti-nd/LDSC/opioid_oaall/$outf.gz\n",
    "\n",
    "\n",
    "## OAall (no deCODE)\n",
    "for chr in {1..22}; do\n",
    "    aws s3 cp  s3://rti-midas-data/studies/ngc/meta/091/processing/oaall/cats+coga+kreek+odb+uhs+vidus+yale-penn.ea.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz . --quiet &\n",
    "done\n",
    "\n",
    "outf=oaall_091.txt\n",
    "for chr in {1..22};do\n",
    "    inf=cats+coga+kreek+odb+uhs+vidus+yale-penn.ea.chr$chr.maf_gt_0.01.rsq_gt_0.3.gz\n",
    "    awk '{print $1,$2,$3,$4,$5,$6,$8}' <(zcat $inf) >> $outf\n",
    "done &\n",
    "\n",
    "gzip $outf\n",
    "## upload to S3\n",
    "aws s3 cp $outf.gz s3://rti-nd/LDSC/opioid_oaall/$outf.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOU\n",
    "`9ec11018-cab4-4da3-8fa4-e2a4b3b335e7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/jmarks/Projects/heroin/ldsc/ngc_all/fou/001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create WorkFlow inputs\n",
    "Here is an example entry in the Excel Phenotype File:\n",
    "\n",
    "**trait\tplot_label\tsumstats_path\tpmid\tcategory\tsample_size\tid_col\tchr_col\tpos_col\teffect_allele_col\tref_allele_col\teffect_col\tpvalue_col\tsample_size_col\teffect_type\tw_ld_chr**\n",
    "```\n",
    "COPDGWAS Hobbs et al.\tCOPD\ts3://rti-nd/LDSC/COPDGWAS_HobbsEtAl/modGcNoOtherMinMissSorted.withchrpos.txt.gz\t28166215\tRespiratory\t51772\t3\t1\t2\t4\t5\t10\t12\t\tbeta\ts3://clustername--files/eur_w_ld_chr.tar.bz2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. upload Excel phenotype file to EC2 instance\n",
    "## 2. then edit full_ld_regression_wf_template.json to include the reference data of choice\n",
    "## 3. lastly use dockerized tool to finish filling out the json file that will be input for workflow\n",
    "\n",
    "## login to a larger compute node\n",
    "qrsh\n",
    "\n",
    "phenD=20191209_heroin_ldsc_phenotypes_local.xlsx\n",
    "procD=/shared/jmarks/heroin/ldsc/ngc_all/fou/001\n",
    "mkdir -p $procD/{ldhub,plot} # for later processing\n",
    "git clone https://github.com/RTIInternational/ld-regression-pipeline/ $procD/ld-regression-pipeline\n",
    "mkdir $procD/ld-regression-pipeline/workflow_inputs\n",
    "## upload files to */workflow_inputs/\n",
    "\n",
    "# create final workflow input (a json file) \n",
    "# edit this file\n",
    "cp $procD/ld-regression-pipeline/json_input/full_ld_regression_wf_template.json \\\n",
    "    $procD/ld-regression-pipeline/workflow_inputs\n",
    "\n",
    "docker run -v $procD/ld-regression-pipeline/workflow_inputs/:/data/ \\\n",
    "    rticode/generate_ld_regression_input_json:1ddbd682cb1e44dab6d11ee571add34bd1d06e21 \\\n",
    "    --json-input /data/full_ld_regression_wf_template.json \\\n",
    "    --pheno-file /data/$phenD >\\\n",
    "        $procD/ld-regression-pipeline/workflow_inputs/final_wf_inputs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## zip appropriate files \n",
    "# Change to directory immediately above metaxcan-pipeline repo\n",
    "cd $procD/ld-regression-pipeline\n",
    "cd ..\n",
    "# Make zipped copy of repo somewhere\n",
    "zip --exclude=*var/* --exclude=*.git/* -r \\\n",
    "    $procD/ld-regression-pipeline/workflow_inputs/ld-regression-pipeline.zip \\\n",
    "    ld-regression-pipeline\n",
    "\n",
    "## copy cromwell config file from S3 to EC2 instance\n",
    "cd /shared/jmarks/bin/cromwell\n",
    "#aws s3 cp s3://rti-cromwell-output/cromwell-config/cromwell_default_genomics_queue.conf .\n",
    "\n",
    "## Run workflowâ€”Navigate to cromwell directory\n",
    "java -Dconfig.file=/shared/jmarks/bin/cromwell/cromwell_default_genomics_queue.conf \\\n",
    "    -jar cromwell-44.jar \\\n",
    "    run $procD/ld-regression-pipeline/workflow/full_ld_regression_wf.wdl \\\n",
    "    -i $procD/ld-regression-pipeline/workflow_inputs/final_wf_inputs.json \\\n",
    "    -p $procD/ld-regression-pipeline/workflow_inputs/ld-regression-pipeline.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record the workflow log-ID. Then get the results on s3 at `s3:///rti-cromwell-output/cromwell-execution/full_ld_regression_wf/<log-ID>/` <br>\n",
    "You can find the log-ID in the directory `/shared/jmarks/bin/cromwell/cromwell-workflow-logs/` (for example).\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LD Hub\n",
    "```\n",
    "Important notes for your uploaded file:\n",
    "\n",
    "1. To save the uploading time, LD Hub only accepts zipped files as input (e.g. mydata.zip).\n",
    "\n",
    "2. Please check that there is ONLY ONE plain TXT file (e.g. mydata.txt) in your zipped file.\n",
    "\n",
    "3. Please make sure you do NOT zip any folder together with the plain txt file (e.g. /myfolder/mydata.txt), otherwise you will get an error: [Errno 2] No such file or directory\n",
    "\n",
    "4. Please do NOT zip multiple files (e.g. zip mydata.zip file1.txt file2.txt ..) or zip a file with in a folder (e.g. zip mydata.zip /path/to/my/file/mydata.txt).\n",
    "\n",
    "5. Please keep the file name of your plain txt file short (less than 50 characters), otherwise you may get an error: [Errno 2] No such file or directory\n",
    "\n",
    "6. Please zip your plain txt file using following command (ONE file at a time):\n",
    "\n",
    "For Windows system: 1) Locate the file that you want to compress. 2) Right-click the file, point to Send to, and then click Compressed (zipped) folder.\n",
    "\n",
    "For Linux and Mac OS system: zip mydata.zip mydata.txt\n",
    "\n",
    "Reminder: for Mac OS system, please do NOT zip you file by right click mouse and click \"Compress\" to zip your file, this will automatically create a folder called \"__MACOS\". You will get an error: [Errno 2] No such file or directory.\n",
    "\n",
    "Upload the trait of interest\n",
    "To save your upload time, we highly recommend you to use the SNP list we used in LD Hub to reduce the number of SNPs in your uploaded file. Click here to download our SNP list (w_hm3.noMHC.snplist.zip).\n",
    "\n",
    "Please upload the zipped file you just created. Click here to download an input example.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $procD/ldhub\n",
    "outF=hiv016_ldhub_with_pvalues.txt # name of file to create for ldhub\n",
    "samp_size=4664\n",
    "\n",
    "### Download outputs for each ref chr from rftm_sumstats step ###\n",
    "aws s3 sync s3://rti-cromwell-output/cromwell-execution/full_ld_regression_wf/71182bae-08fb-4733-825e-85f1fdec8f81/call-munge_ref/MUNGE_REF_WF.munge_sumstats_wf/5b22c4ab-015b-4c87-b43d-4b8866bc6e54/call-munge_chr_wf/ .\n",
    "        \n",
    "mv  */MUNGE_CHR.munge_sumstats_chr_wf/*/call-rfmt_sumstats/*.standardized.phase3ID.munge_ready.txt .\n",
    "rm -rf shard*\n",
    "\n",
    "## Concat into single file ##\n",
    "cat *.chr1.*.standardized.phase3ID.munge_ready.txt > $outF\n",
    "for chr in {2..22}; do\n",
    "    tail -n +2  *.chr$chr.*.standardized.phase3ID.munge_ready.txt >> $outF\n",
    "done\n",
    "\n",
    "## Remove unnecessary columns (need snpID, A1, A2 Beta, Pvalue) in that order ##\n",
    "head -1 $outF | cut -f1,4,5,6,7 > tmp\n",
    "tail -n +2 $outF | awk 'BEGIN{OFS=\"\\t\"}{print $1, $4, $5, $6, $7}'  >> tmp && mv tmp $outF\n",
    "\n",
    "## Add sample size column (sample = 18245.00) and change header names ##\n",
    "cat $outF | awk -v var=$samp_size -F \"\\t\"  \\\n",
    "    'BEGIN{OFS=\"\\t\";} NR==1{print \"snpid\", \"A1\", \"A2\", \"BETA\", \"N\", \"P-value\"} \\\n",
    "    NR>1{print $1,$2,$3,$4,var, $5}' > tmp && mv tmp $outF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/jmarks/Projects/HIV/ldsc/meta016_copd_pft/v02/processing/input/ldhub\n",
    "scp -i ~/.ssh/gwas_rsa   ec2-user@54.84.72.140:/shared/jmarks/proj/hiv/ldsc/meta016/v02/ldhub/hiv016_ldhub_with_pvalues.txt\n",
    "    \n",
    "# zip file with 7zip\n",
    "hiv016_ldhub_with_pvalues.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload input file\n",
    "Follow the steps above to zip and upload input file. Essentially, \n",
    "* download the file created in the cell above to your local machine.\n",
    "* Then zip this file (and only this file).\n",
    "* Login to [LDHub](http://ldsc.broadinstitute.org/ldhub/) by clicking on `Get Started with LDHub` and then sign in with your Google email account.\n",
    "* Click `Go Test Center`\n",
    "* Click `Continue`\n",
    "* Upload zipped file by clicking `Choose File`, naming your trait, and clicking `Continue`.\n",
    "* Select traits of interest from LDHub by checking the box next to the trait of interest and then clicking `Submit your request`\n",
    "\n",
    "**Note**: keep browser open during LDSC analysis on LDHub.\n",
    "\n",
    "**title:** `hiv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Final Plot\n",
    "The merged CSV file should have the header:\n",
    "```\n",
    "trait2\tTrait_Label\tTrait_Group\trg\tse\tz\tp\th2_obs\th2_obs_se\th2_int\th2_int_se\tgcov_int\tgcov_int_se\n",
    "```\n",
    "\n",
    "**Note**: upload the plot table to EC2 instance to run docker and create the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enter interactive mode ##\n",
    "# note that the image tag corresponds to the latest tag for this image\n",
    "docker run -it -v\"/shared/jmarks/proj/hiv/ldsc/meta016/v02/plot:/data/\" \\\n",
    "    rticode/plot_ld_regression_results:7bbd11a1d0c664bcb8bede8c398772b13abe15b3  /bin/bash\n",
    "    \n",
    "Rscript /opt/plot_ld_regression/plot_ld_regression_results.R  \\\n",
    "    --input_file 20190925_hivacq_ldsc_meta016_copd_pft_rg_results.csv \\\n",
    "    --output_file 20190925_hivacq_ldsc_meta016_copd_pft_rg_results.pdf  \\\n",
    "    --comma_delimited \\\n",
    "    --title \"HIV Acquisition Meta016\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
